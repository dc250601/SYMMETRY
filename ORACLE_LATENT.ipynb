{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56abc883-f992-4707-9a4b-6941e45bb2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/diptarko/miniconda3/envs/work/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import imutils\n",
    "import math\n",
    "\n",
    "import data\n",
    "import models\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58e380a-ae22-4ce2-8505-4835f1857747",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "LATENT_DIM = 64\n",
    "NUM_GENERATORS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1e9bce-fb18-4a34-87e4-5e745626c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will work with the distilled data for the sake of simplicity of our work, it turns out that the distilled dataset contains only 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc7e2c5-0b9f-4e7e-9b22-328a7fb8b30e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data.get_dataset_distilled()\n",
    "\n",
    "train_dataset = TensorDataset(X_train,Y_train)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=512,\n",
    "                              shuffle=True,\n",
    "                              drop_last = True,\n",
    "                              num_workers=8,\n",
    "                              pin_memory = False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test,Y_test)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              batch_size=512,\n",
    "                              shuffle=True,\n",
    "                              drop_last = True,\n",
    "                              num_workers=8,\n",
    "                              pin_memory = False)\n",
    "\n",
    "model_VAE = torch.load(\"../symmetry_2/VAE.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c62e46-a374-4b52-a71d-644d28b2784a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_VAE.eval()\n",
    "train_Z = model_VAE.fc2(model_VAE.fc_mu(model_VAE.encoder(X_train.to(device)))).cpu().detach()\n",
    "test_Z = model_VAE.fc2(model_VAE.fc_mu(model_VAE.encoder(X_test.to(device)))).cpu().detach()\n",
    "\n",
    "train_dataset_Z = TensorDataset(train_Z,Y_train)\n",
    "train_dataloader_Z = DataLoader(train_dataset_Z,\n",
    "                              batch_size=512,\n",
    "                              shuffle=True,\n",
    "                              drop_last = True,\n",
    "                              num_workers=8,\n",
    "                              pin_memory = False)\n",
    "\n",
    "test_dataset_Z = TensorDataset(test_Z,Y_test)\n",
    "test_dataloader_Z = DataLoader(test_dataset_Z,\n",
    "                              batch_size=512,\n",
    "                              shuffle=True,\n",
    "                              drop_last = True,\n",
    "                              num_workers=8,\n",
    "                              pin_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa0e0f2-712b-485e-9200-894184a40552",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/diptarko/miniconda3/envs/work/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model_fe = models.MLP(feature_size=LATENT_DIM).to(device)\n",
    "model_fd = models.MLP(feature_size=LATENT_DIM).to(device)\n",
    "# model_fo = models.LatentOracle().to(device)\n",
    "model_fo = models.LatentDescriminator().to(device) # Since we ony have two elements\n",
    "\n",
    "model_symmetry = models.GroupLatent(num_features=LATENT_DIM,num_generators=NUM_GENERATORS).to(device)\n",
    "\n",
    "optimiser_fe = torch.optim.Adam(model_fe.parameters(), lr = 1e-3)\n",
    "optimiser_fd = torch.optim.Adam(model_fd.parameters(), lr = 1e-3)\n",
    "optimiser_fo = torch.optim.Adam(model_fo.parameters(), lr = 1e-3)\n",
    "optimiser_symmetry = torch.optim.Adam(model_symmetry.parameters(), lr = 1e-3)\n",
    "\n",
    "\n",
    "criterion_mse = nn.MSELoss()\n",
    "criterion_BCE = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5812bc9-81d9-4244-bf98-df92b5c99288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/227 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (14336x28 and 64x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m optimiser_symmetry\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m theta \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrand(Z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],device \u001b[38;5;241m=\u001b[39m device) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_GENERATORS)]  \u001b[38;5;66;03m#Sampling\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m P \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m P_S \u001b[38;5;241m=\u001b[39m model_symmetry(theta \u001b[38;5;241m=\u001b[39m theta, x \u001b[38;5;241m=\u001b[39m P)\n\u001b[1;32m     15\u001b[0m Z_S \u001b[38;5;241m=\u001b[39m model_fd(P_S)\n",
      "File \u001b[0;32m~/miniconda3/envs/work/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/work/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pscratch/sd/d/diptarko/SYMMETRY/models.py:411\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 411\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# x = self.bn1(x) #Commenting for now will remove later to increase expressivity\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/work/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/work/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/work/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (14336x28 and 64x64)"
     ]
    }
   ],
   "source": [
    "loss_S_closure = []\n",
    "loss_S_orth = []\n",
    "loss_S_collapse = []\n",
    "\n",
    "loss_space = []\n",
    "loss_oracle = []\n",
    "for i in range(300):\n",
    "    \n",
    "    loss_S_closure_ = 0\n",
    "    loss_S_orth_ = 0\n",
    "    loss_S_collapse_ = 0\n",
    "\n",
    "    loss_space_ = 0\n",
    "    loss_oracle_ = 0\n",
    "    \n",
    "    for Z,M in tqdm(train_dataloader):\n",
    "        Z = Z.to(device)\n",
    "        M = M.to(device)\n",
    "        \n",
    "        optimiser_fd.zero_grad()\n",
    "        optimiser_fe.zero_grad()\n",
    "        optimiser_fo.zero_grad()\n",
    "        optimiser_symmetry.zero_grad()\n",
    "\n",
    "        theta = [(2*torch.rand(Z.shape[0],device = device) - 1) for i in range(NUM_GENERATORS)]  #Sampling\n",
    "\n",
    "        P = model_fe(Z)\n",
    "        P_S = model_symmetry(theta = theta, x = P)\n",
    "        Z_S = model_fd(P_S)\n",
    "        m = model_fo(Z)\n",
    "        m_S = model_fo(Z_S)\n",
    "        Z_P = model_fd(P)\n",
    "\n",
    "        loss1 = criterion_BCE(m_S,torch.sigmoid(m))\n",
    "        loss2 = model_symmetry.orthogonal_loss()\n",
    "        loss3 = model_symmetry.collapse_loss()\n",
    "        loss5 = criterion_mse(Z,Z_P)\n",
    "        loss6 = criterion_BCE(m.squeeze(),M)\n",
    "        \n",
    "        loss_S = loss1 + loss2 + loss3\n",
    "        loss_Ae = loss5\n",
    "        loss_O = loss6\n",
    "        \n",
    "        loss_S.backward(retain_graph=True)\n",
    "        loss_Ae.backward()\n",
    "        loss_O.backward()\n",
    "        \n",
    "        \n",
    "        \n",
    "        optimiser_fd.step()\n",
    "        optimiser_fe.step()\n",
    "        optimiser_fo.step()\n",
    "        optimiser_symmetry.step()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9add365b-d2ac-4258-ba5a-1fbadc441c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Z,M  =  next(iter(train_dataloader_Z))\n",
    "Z = Z.to(device)\n",
    "M = M.to(device)\n",
    "\n",
    "optimiser_fd.zero_grad()\n",
    "optimiser_fe.zero_grad()\n",
    "optimiser_fo.zero_grad()\n",
    "optimiser_symmetry.zero_grad()\n",
    "\n",
    "theta = [(2*torch.rand(Z.shape[0],device = device) - 1) for i in range(NUM_GENERATORS)]  #Sampling\n",
    "\n",
    "P = model_fe(Z)\n",
    "P_S = model_symmetry(theta = theta, x = P)\n",
    "Z_S = model_fd(P_S)\n",
    "m = model_fo(Z)\n",
    "m_S = model_fo(Z_S)\n",
    "Z_P = model_fd(P)\n",
    "\n",
    "loss1 = criterion_BCE(m_S,torch.sigmoid(m))\n",
    "loss2 = model_symmetry.orthogonal_loss()\n",
    "loss3 = model_symmetry.collapse_loss()\n",
    "loss5 = criterion_mse(Z,Z_P)\n",
    "loss6 = criterion_BCE(m.squeeze(),M)\n",
    "\n",
    "loss_S = loss1 + loss2 + loss3\n",
    "loss_Ae = loss5\n",
    "loss_O = loss6\n",
    "\n",
    "loss_S.backward(retain_graph=True)\n",
    "loss_Ae.backward()\n",
    "loss_O.backward()\n",
    "\n",
    "optimiser_fd.step()\n",
    "optimiser_fe.step()\n",
    "optimiser_fo.step()\n",
    "optimiser_symmetry.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa173664-19ad-42b9-a1f8-3bdde4ce67da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7070, device='cuda:0',\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1.to(loss1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099c172-68f4-48e4-ad38-eeac19d8e665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e543f-583a-4ad9-ae44-afd2823ca161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.sigmoid(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cee87a-5054-4b31-ac21-18a45c3bf243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_symmetry.group[0].algebra.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2a7ef5-ba4d-4718-b0fd-78b49bccf104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5ae5f-73f9-4c37-8bf8-f3d0ced94a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_conda",
   "language": "python",
   "name": "work_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
